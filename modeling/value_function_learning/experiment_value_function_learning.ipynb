{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ==This is just temporary using experiment results as data set. Need to generate one==",
   "id": "1daa3550e1fb446e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "40fee80cfd31c44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_processing.process_experiments_results import experiments_results_to_empirical_vd_table, \\\n",
    "    add_state_likelihood_column, vd_table_to_empirical_voa_table\n",
    "\n",
    "results_tables = {\n",
    "    'planner_2000': '../../experiments_sim/results/experiments_planner_2000_results.csv',\n",
    "    'planner_200': '../../experiments_sim/results/experiments_planner_200_results.csv',\n",
    "    'handmade': '../../experiments_sim/results/experiments_handmade_results.csv',\n",
    "}\n",
    "\n",
    "dataframes = {\n",
    "    key: pd.read_csv(value) for key, value in results_tables.items()\n",
    "}\n",
    "\n",
    "vd_tables = {\n",
    "    key: experiments_results_to_empirical_vd_table(value) for key, value in dataframes.items()\n",
    "}\n",
    "# add state probability column\n",
    "vd_tables = {\n",
    "    key: add_state_likelihood_column(value) for key, value in vd_tables.items()\n",
    "}\n",
    "\n",
    "# compute empirical VOA table\n",
    "voa_tables = {\n",
    "    key: vd_table_to_empirical_voa_table(value) for key, value in vd_tables.items()\n",
    "}\n",
    "voa_tables['planner_200']"
   ],
   "id": "9d926af69f850376"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# generate data, take only rows with unique belief idx and take \"belief_mus\" and \"belief_sigmas\" and empirical_baseline_value\n",
    "data = voa_tables['planner_200'][['belief_idx', 'belief_mus', 'belief_sigmas', 'empirical_baseline_value']]\n",
    "data = data.drop_duplicates(subset='belief_idx')\n",
    "\n",
    "from ast import literal_eval\n",
    "# Convert string representations to lists\n",
    "data['belief_mus'] = data['belief_mus'].apply(literal_eval)\n",
    "data['belief_sigmas'] = data['belief_sigmas'].apply(literal_eval)\n",
    "data"
   ],
   "id": "bd44ff1f49962ed8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = np.array([np.array(mu + sigma).flatten() for mu, sigma in zip(data['belief_mus'], data['belief_sigmas'])])\n",
    "y = data['empirical_baseline_value'].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)"
   ],
   "id": "dec739ae2eaad856"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create dictionary of models to test\n",
    "models = {\n",
    "    'Mean Baseline': DummyRegressor(strategy='mean'),\n",
    "    'Median Baseline': DummyRegressor(strategy='median'),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    rmse_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    mae_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    rmse_scores = np.sqrt(-rmse_scores)  # Convert MSE to RMSE\n",
    "    mae_scores = -mae_scores  # Convert negative MAE to positive\n",
    "\n",
    "    results[name] = {\n",
    "        'mean_rmse': rmse_scores.mean(),\n",
    "        'std_rmse': rmse_scores.std(),\n",
    "        'mean_mae': mae_scores.mean(),\n",
    "        'std_mae': mae_scores.std()\n",
    "    }\n",
    "\n",
    "# Print results sorted by mean MAE\n",
    "print(\"Results sorted by Mean MAE:\\n\")\n",
    "sorted_results = dict(sorted(results.items(), key=lambda x: x[1]['mean_mae']))\n",
    "for name, metrics in sorted_results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean RMSE: {metrics['mean_rmse']:.4f} (±{metrics['std_rmse']:.4f})\")\n",
    "    print(f\"  Mean MAE:  {metrics['mean_mae']:.4f} (±{metrics['std_mae']:.4f})\")\n",
    "    print()"
   ],
   "id": "16b8bda3e5d277ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Next:\n",
    "* More data, use original data and take belief after help.\n",
    "* augmentation: scramble blocks"
   ],
   "id": "cb30c7c3801449e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "596e2b2ed4560c21"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
