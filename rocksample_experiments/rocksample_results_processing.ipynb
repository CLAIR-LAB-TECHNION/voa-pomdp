{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results_file = 'configurations/RS11_11_cluster5_4_merged.json'"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def json_results_to_dataframe(json_data):\n",
    "    experiments = []\n",
    "    metadata = json_data['metadata']\n",
    "\n",
    "    for exp in json_data['experiments']:\n",
    "        # Skip experiments with no results\n",
    "        if exp['total_reward'] is None or exp['total_discounted_reward'] is None:\n",
    "            continue\n",
    "\n",
    "        # For each run in the experiment\n",
    "        for run_idx, (total_reward, discounted_reward) in enumerate(zip(\n",
    "            exp['total_reward'], exp['total_discounted_reward'])):\n",
    "\n",
    "            exp_data = {\n",
    "                'experiment_id': exp['experiment_id'],\n",
    "                'env_instance_id': exp['env_instance_id'],\n",
    "                'state_id': exp['state_id'],\n",
    "                'help_config_id': exp['help_config_id'],\n",
    "                'rover_position': str(exp['rover_position']),\n",
    "                'rock_locations': str(exp['rock_locations']),\n",
    "                'rock_types': str(exp['rock_types']),\n",
    "                'help_actions': str(exp['help_actions']),\n",
    "                'total_reward': total_reward,\n",
    "                'total_discounted_reward': discounted_reward,\n",
    "                'run_id': run_idx\n",
    "            }\n",
    "            experiments.append(exp_data)\n",
    "\n",
    "    df = pd.DataFrame(experiments)\n",
    "    return df, metadata"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(results_file) as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "df, metadata = json_results_to_dataframe(json_data)\n",
    "df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "instances_count = df.groupby('env_instance_id').agg({\n",
    "    'state_id': 'nunique',\n",
    "    'help_config_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Check which instances have complete experiments (15 states and at least 10 help configs) (that's almost complete instances)\n",
    "complete_instances = instances_count[\n",
    "    (instances_count['state_id'] >= 20) &\n",
    "    (instances_count['help_config_id'] >= 10)\n",
    "]['env_instance_id'].tolist()\n",
    "\n",
    "print(f\"Number of instances: {len(instances_count)}\")\n",
    "print(f\"Number of complete instances: {len(complete_instances)}\")\n",
    "\n",
    "# Filter for complete instances only\n",
    "df_complete = df[df['env_instance_id'].isin(complete_instances)]\n",
    "\n",
    "# Count experiments with less than 3 runs\n",
    "experiment_runs = df_complete.groupby(['env_instance_id', 'state_id', 'help_config_id']).size().reset_index(name='run_count')\n",
    "\n",
    "print(f\"\\nNumber of experiments with at least 3 runs: {len(experiment_runs[experiment_runs['run_count'] >= 3])}\")\n",
    "print(f\"Number of experiments with less than 3 runs: {len(experiment_runs[experiment_runs['run_count'] < 3])}\")\n",
    "print(f\"Number of experiments with less than 2 runs: {len(experiment_runs[experiment_runs['run_count'] < 2])}\")\n",
    "print(f\"Number of experiments with less than 1 runs: {len(experiment_runs[experiment_runs['run_count'] < 1])}\")\n",
    "\n",
    "env_instances_ids = df_complete['env_instance_id'].unique()\n",
    "print(f\"complete instances: {env_instances_ids}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# env_instances_to_analyze = [0,]\n",
    "env_instances_to_analyze = env_instances_ids\n",
    "\n",
    "df_complete = df_complete[df_complete['env_instance_id'].isin(env_instances_to_analyze)]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_discounted_stds():\n",
    "   # Calculate std across runs for each (env_instance, help_config, state) combination\n",
    "   policy_std = df_complete.groupby(['env_instance_id', 'help_config_id', 'state_id'])['total_discounted_reward'].std().reset_index(name='policy_std')\n",
    "\n",
    "   # Calculate std across states for each (env_instance, help_config) combination\n",
    "   state_std = df_complete.groupby(['env_instance_id', 'help_config_id'])['total_discounted_reward'].std().reset_index(name='state_std')\n",
    "\n",
    "   # Create plots\n",
    "   plt.figure(figsize=(15, 5))\n",
    "\n",
    "   # Plot 1: Distribution of policy stds\n",
    "   plt.subplot(131)\n",
    "   sns.histplot(policy_std['policy_std'].dropna(), bins=10)\n",
    "   plt.title('Std Dev Across Policy Runs\\n(Discounted Reward)')\n",
    "   plt.xlabel('Standard Deviation')\n",
    "\n",
    "   # Plot 2: Distribution of state stds\n",
    "   plt.subplot(132)\n",
    "   sns.histplot(state_std['state_std'].dropna(), bins=10)\n",
    "   plt.title('Std Dev Across States\\n(Discounted Reward)')\n",
    "   plt.xlabel('Standard Deviation')\n",
    "\n",
    "   # Plot 3: Distribution of rewards\n",
    "   plt.subplot(133)\n",
    "   sns.histplot(df_complete['total_discounted_reward'], bins=30)\n",
    "   plt.title('Distribution of Discounted Rewards')\n",
    "   plt.xlabel('Discounted Reward')\n",
    "\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "   # Print statistics\n",
    "   print(\"\\nPolicy StdDev Statistics (Discounted Reward):\")\n",
    "   print(policy_std['policy_std'].describe())\n",
    "   print(\"\\nState StdDev Statistics (Discounted Reward):\")\n",
    "   print(state_std['state_std'].describe())\n",
    "\n",
    "   # Calculate the ratio of standard deviations\n",
    "   mean_policy_std = policy_std['policy_std'].mean()\n",
    "   mean_state_std = state_std['state_std'].mean()\n",
    "   print(f\"\\nRatio of mean state stddev to mean policy stddev: {mean_state_std/mean_policy_std:.2f}\")\n",
    "\n",
    "   print(\"\\nOverall Discounted Reward Statistics:\")\n",
    "   print(df_complete['total_discounted_reward'].describe())\n",
    "\n",
    "analyze_discounted_stds()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a subplot grid based on number of environments\n",
    "n_envs = len(env_instances_to_analyze)\n",
    "n_cols = 4  # We can adjust this number to change layout\n",
    "n_rows = (n_envs + n_cols - 1) // n_cols  # Ceiling division to get required rows\n",
    "\n",
    "plt.figure(figsize=(15, 3*n_rows))\n",
    "\n",
    "for idx, env_id in enumerate(env_instances_to_analyze):\n",
    "    plt.subplot(n_rows, n_cols, idx + 1)\n",
    "    env_data = df_complete[df_complete['env_instance_id'] == env_id]\n",
    "\n",
    "    sns.histplot(data=env_data, x='total_discounted_reward', bins=20)\n",
    "    plt.title(f'Env Instance {env_id}')\n",
    "    plt.xlabel('Discounted Reward')\n",
    "\n",
    "    # Add mean and std as text\n",
    "    mean_reward = env_data['total_discounted_reward'].mean()\n",
    "    std_reward = env_data['total_discounted_reward'].std()\n",
    "    plt.text(0.05, 0.95, f'μ={mean_reward:.1f}\\nσ={std_reward:.1f}',\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for each environment\n",
    "print(\"\\nSummary statistics per environment:\")\n",
    "print(df_complete.groupby('env_instance_id')['total_discounted_reward'].describe())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute VD and VOA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_value_differences(df):\n",
    "    # First average across runs for each experiment configuration\n",
    "    baseline_means = df[df['help_config_id'] == -1].groupby(\n",
    "        ['env_instance_id', 'state_id'])['total_discounted_reward'].mean().reset_index()\n",
    "\n",
    "    value_diff_results = []\n",
    "    for _, baseline_row in baseline_means.iterrows():\n",
    "        env_instance_id = baseline_row['env_instance_id']\n",
    "        state_id = baseline_row['state_id']\n",
    "        baseline_reward = baseline_row['total_discounted_reward']  # This is now mean across runs\n",
    "\n",
    "        # Get mean rewards for help experiments while preserving help_actions\n",
    "        help_exps = df[\n",
    "            (df['env_instance_id'] == env_instance_id) &\n",
    "            (df['state_id'] == state_id) &\n",
    "            (df['help_config_id'] != -1)\n",
    "        ].groupby(['env_instance_id', 'state_id', 'help_config_id']\n",
    "        ).agg({\n",
    "            'total_discounted_reward': 'mean',\n",
    "            'help_actions': 'first',\n",
    "            'rover_position': 'first',\n",
    "            'rock_locations': 'first',\n",
    "            'rock_types': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        for _, help_row in help_exps.iterrows():\n",
    "            result_dict = help_row.to_dict()\n",
    "            result_dict['value_diff'] = help_row['total_discounted_reward'] - baseline_reward\n",
    "            result_dict['baseline_value'] = baseline_reward\n",
    "            value_diff_results.append(result_dict)\n",
    "\n",
    "    return pd.DataFrame(value_diff_results)\n",
    "\n",
    "def compute_empirical_voa(vd_df):\n",
    "    # Since states are sampled uniformly, we can just average over states\n",
    "    # Group by env_instance_id and help_config_id\n",
    "    voa_results = []\n",
    "\n",
    "    for (env_id, help_config), group in vd_df.groupby(['env_instance_id', 'help_config_id']):\n",
    "        # Compute mean value difference (VOA)\n",
    "        empirical_voa = group['value_diff'].mean()\n",
    "        # Compute variance\n",
    "        empirical_variance = group['value_diff'].var()\n",
    "\n",
    "        voa_results.append({\n",
    "            'env_instance_id': env_id,\n",
    "            'help_config_id': help_config,\n",
    "            'help_actions': group['help_actions'].iloc[0],\n",
    "            'rover_position': group['rover_position'].iloc[0],\n",
    "            'rock_locations': group['rock_locations'].iloc[0],\n",
    "            'empirical_voa': empirical_voa,\n",
    "            'empirical_voa_variance': empirical_variance,\n",
    "            'n_states': len(group),\n",
    "            'baseline_value': group['baseline_value'].mean()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(voa_results)\n",
    "\n",
    "vd_df = compute_value_differences(df_complete)\n",
    "voa_df = compute_empirical_voa(vd_df)\n",
    "\n",
    "print(\"\\nValue Difference Statistics:\")\n",
    "print(vd_df['value_diff'].describe())\n",
    "\n",
    "print(\"\\nEmpirical VOA Statistics:\")\n",
    "print(voa_df['empirical_voa'].describe())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot 1: Distribution of Value Differences\n",
    "plt.subplot(121)\n",
    "sns.histplot(vd_df['value_diff'], bins=30)\n",
    "plt.title('Distribution of Value Differences')\n",
    "plt.xlabel('Value Difference')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot 2: Distribution of VOA\n",
    "plt.subplot(122)\n",
    "sns.histplot(voa_df['empirical_voa'], bins=20)\n",
    "plt.title('Distribution of Empirical VOA')\n",
    "plt.xlabel('VOA')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nValue Difference Statistics:\")\n",
    "print(vd_df['value_diff'].describe())\n",
    "print(\"\\nVOA Statistics:\")\n",
    "print(voa_df['empirical_voa'].describe())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a subplot grid based on number of environments\n",
    "n_envs = len(env_instances_to_analyze)\n",
    "n_cols = 4  # We can adjust this number to change layout\n",
    "n_rows = (n_envs + n_cols - 1) // n_cols  # Ceiling division to get required rows\n",
    "\n",
    "plt.figure(figsize=(15, 3*n_rows))\n",
    "\n",
    "for idx, env_id in enumerate(env_instances_to_analyze):\n",
    "   plt.subplot(n_rows, n_cols, idx + 1)\n",
    "   env_data = voa_df[voa_df['env_instance_id'] == env_id]\n",
    "\n",
    "   sns.histplot(data=env_data, x='empirical_voa', bins=20)\n",
    "   plt.title(f'Env Instance {env_id}')\n",
    "   plt.xlabel('VOA')\n",
    "\n",
    "   # Add mean and std as text\n",
    "   mean_voa = env_data['empirical_voa'].mean()\n",
    "   std_voa = env_data['empirical_voa'].std()\n",
    "   plt.text(0.05, 0.95, f'μ={mean_voa:.1f}\\nσ={std_voa:.1f}',\n",
    "            transform=plt.gca().transAxes,\n",
    "            verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for each environment\n",
    "print(\"\\nSummary statistics of VOA per environment:\")\n",
    "print(voa_df.groupby('env_instance_id')['empirical_voa'].describe())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Confidence"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_voa_with_standard_errors(voa_df):\n",
    "    # Compute standard error\n",
    "    voa_df['std_error'] = np.sqrt(voa_df['empirical_voa_variance'] / voa_df['n_states'])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Create labels for each env-help pair\n",
    "    labels = [f\"Env{row['env_instance_id']}-Help{row['help_config_id']}\"\n",
    "             for _, row in voa_df.iterrows()]\n",
    "\n",
    "    plt.errorbar(range(len(voa_df)), voa_df['empirical_voa'],\n",
    "                yerr=voa_df['std_error'], fmt='o', alpha=0.5)\n",
    "    plt.xticks(range(len(voa_df)), labels, rotation=45)\n",
    "    plt.xlabel('Environment-Help Configuration')\n",
    "    plt.ylabel('VOA with Standard Error')\n",
    "    plt.title('VOA with Standard Error Bars')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bootstrap_confidence_intervals(vd_df, n_bootstrap=1000):\n",
    "   results = []\n",
    "   ci_sizes = {'95': [], '90': [], '80': []}\n",
    "\n",
    "   percentiles = {\n",
    "       '95': [2.5, 97.5],\n",
    "       '90': [5, 95],\n",
    "       '80': [10, 90]\n",
    "   }\n",
    "\n",
    "   for (env_id, help_config), group in vd_df.groupby(['env_instance_id', 'help_config_id']):\n",
    "       boots = [group['value_diff'].sample(n=len(group), replace=True).mean()\n",
    "               for _ in range(n_bootstrap)]\n",
    "       mean_voa = group['value_diff'].mean()\n",
    "\n",
    "       # Calculate CIs for each confidence level\n",
    "       cis = {}\n",
    "       ci_size = {}\n",
    "       for conf_level, percs in percentiles.items():\n",
    "           ci_low, ci_high = np.percentile(boots, percs)\n",
    "           cis[conf_level] = (ci_low, ci_high)\n",
    "           ci_size[conf_level] = ci_high - ci_low\n",
    "           ci_sizes[conf_level].append(ci_size[conf_level])\n",
    "\n",
    "       results.append({\n",
    "           'env_instance_id': env_id,\n",
    "           'help_config_id': help_config,\n",
    "           'voa': mean_voa,\n",
    "           'ci_low_95': cis['95'][0],\n",
    "           'ci_high_95': cis['95'][1],\n",
    "           'ci_size_95': ci_size['95'],\n",
    "           'ci_low_90': cis['90'][0],\n",
    "           'ci_high_90': cis['90'][1],\n",
    "           'ci_size_90': ci_size['90'],\n",
    "           'ci_low_80': cis['80'][0],\n",
    "           'ci_high_80': cis['80'][1],\n",
    "           'ci_size_80': ci_size['80']\n",
    "       })\n",
    "\n",
    "   results_df = pd.DataFrame(results)\n",
    "\n",
    "   # Print statistics for each confidence level\n",
    "   for conf_level in ['95', '90', '80']:\n",
    "       print(f\"\\n{conf_level}% Confidence Interval Size Statistics:\")\n",
    "       print(f\"Mean CI size: {np.mean(ci_sizes[conf_level]):.2f}\")\n",
    "       print(f\"Median CI size: {np.median(ci_sizes[conf_level]):.2f}\")\n",
    "       print(f\"Std of CI sizes: {np.std(ci_sizes[conf_level]):.2f}\")\n",
    "       print(f\"Min CI size: {np.min(ci_sizes[conf_level]):.2f}\")\n",
    "       print(f\"Max CI size: {np.max(ci_sizes[conf_level]):.2f}\")\n",
    "\n",
    "   # Plot with all confidence levels\n",
    "   plt.figure(figsize=(12, 6))\n",
    "   labels = [f\"Env{row['env_instance_id']}-Help{row['help_config_id']}\"\n",
    "            for _, row in results_df.iterrows()]\n",
    "\n",
    "   # Plot different confidence intervals with different colors/styles\n",
    "   plt.errorbar(range(len(results_df)), results_df['voa'],\n",
    "               yerr=[results_df['voa'] - results_df['ci_low_95'],\n",
    "                     results_df['ci_high_95'] - results_df['voa']],\n",
    "               fmt='o', alpha=0.3, color='blue', label='95% CI')\n",
    "   plt.errorbar(range(len(results_df)), results_df['voa'],\n",
    "               yerr=[results_df['voa'] - results_df['ci_low_90'],\n",
    "                     results_df['ci_high_90'] - results_df['voa']],\n",
    "               fmt='o', alpha=0.5, color='green', label='90% CI')\n",
    "   plt.errorbar(range(len(results_df)), results_df['voa'],\n",
    "               yerr=[results_df['voa'] - results_df['ci_low_80'],\n",
    "                     results_df['ci_high_80'] - results_df['voa']],\n",
    "               fmt='o', alpha=0.7, color='red', label='80% CI')\n",
    "\n",
    "   plt.xticks(range(len(results_df)), labels, rotation=45)\n",
    "   plt.xlabel('Environment-Help Configuration')\n",
    "   plt.ylabel('VOA with CIs')\n",
    "   plt.title('VOA with Different Confidence Intervals')\n",
    "   plt.legend()\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "   return results_df, ci_sizes"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot standard error visualization\n",
    "plot_voa_with_standard_errors(voa_df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot and get bootstrap CI visualization\n",
    "bootstrap_results, ci_sizes = plot_bootstrap_confidence_intervals(vd_df, n_bootstrap=1000)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for conf_level in ['95', '90', '80']:\n",
    "    print(f\"\\n{conf_level}% Confidence Interval Size Statistics:\")\n",
    "    print(f\"Mean CI size: {np.mean(ci_sizes[conf_level]):.2f}\")\n",
    "    print(f\"Median CI size: {np.median(ci_sizes[conf_level]):.2f}\")\n",
    "    print(f\"Std of CI sizes: {np.std(ci_sizes[conf_level]):.2f}\")\n",
    "    print(f\"Min CI size: {np.min(ci_sizes[conf_level]):.2f}\")\n",
    "    print(f\"Max CI size: {np.max(ci_sizes[conf_level]):.2f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# add the confidence intervals to the voa_df\n",
    "voa_df = voa_df.merge(bootstrap_results, on=['env_instance_id', 'help_config_id'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Huristics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from rocksample_experiments.heuristics import h_first_step_planning_value_diff, h_rollout_policy_value, h_vd_results\n",
    "from rocksample_experiments.heuristics_evaluation import test_heuristic_on_problem_instance, heuristic_metrics, test_heuristic_on_all_instances_parallel, calculate_mean_metrics\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_test_results(results_dfs_dict, dir_path):\n",
    "    # make sure the directory exists\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    for i in results_dfs_dict:\n",
    "        results_dfs_dict[i].to_csv(f'{dir_path}/env{i}.csv')\n",
    "\n",
    "def load_test_results(dir_path):\n",
    "    results_dfs_dict = {}\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            env_id = int(file.split('env')[1].split('.')[0])\n",
    "            results_dfs_dict[env_id] = pd.read_csv(f'{dir_path}/{file}', index_col=0)\n",
    "    return results_dfs_dict"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline mc simulations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_states = [1, 2, 3, 5, 10, 25, 50]\n",
    "\n",
    "for n in n_states:\n",
    "    heuristic_kwargs = {'vd_table': vd_df, 'n_states_to_use': n}\n",
    "    results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_vd_results, n_jobs=10, heuristic_kwargs=heuristic_kwargs)\n",
    "    print(\"--------------------\")\n",
    "    print(f\"n_states: {n}\")\n",
    "    print(calculate_mean_metrics(results_dfs_dict))\n",
    "    print(\"--------------------\")\n",
    "    save_test_results(results_dfs_dict, f'heuristics_evaluation/vd_results_n_states_{n}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### First step planning value difference"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "uncomment to recalculate:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Uncomment to recalculate\n",
    "\n",
    "# results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_first_step_planning_value_diff, n_jobs=10)\n",
    "# save_test_results(results_dfs_dict, 'heuristics_evaluation/first_step_planning')\n",
    "\n",
    "heuristic_kwargs = {'n_trials': 10}\n",
    "results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_first_step_planning_value_diff, n_jobs=10\n",
    "                                                            , heuristic_kwargs=heuristic_kwargs)\n",
    "save_test_results(results_dfs_dict, 'heuristics_evaluation/first_step_planning_10_trials')\n",
    "\n",
    "heuristic_kwargs = {'n_sims': 20000, 'max_depth': 100}\n",
    "results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_first_step_planning_value_diff, n_jobs=10\n",
    "                                                            , heuristic_kwargs=heuristic_kwargs)\n",
    "save_test_results(results_dfs_dict, 'heuristics_evaluation/first_step_planning_20000_sims_100_depth')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_plain_first_step_planning = load_test_results('heuristics_evaluation/first_step_planning')\n",
    "print(calculate_mean_metrics(results_plain_first_step_planning))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_first_step_planning_10_trials = load_test_results('heuristics_evaluation/first_step_planning_10_trials')\n",
    "print(calculate_mean_metrics(results_first_step_planning_10_trials))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_first_step_planning_20000_sims_100_depth = load_test_results('heuristics_evaluation/first_step_planning_20000_sims_100_depth')\n",
    "print(calculate_mean_metrics(results_first_step_planning_20000_sims_100_depth))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rollout policy value"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Uncomment to recalculate\n",
    "\n",
    "# heuristic_kwargs = {'n_rollouts': 10}\n",
    "# results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_rollout_policy_value, n_jobs=10,\n",
    "#                                                             heuristic_kwargs=heuristic_kwargs)\n",
    "# save_test_results(results_dfs_dict, 'heuristics_evaluation/rollout_policy_value_10_rollouts')\n",
    "#\n",
    "# heuristic_kwargs = {'n_rollouts': 100}\n",
    "# results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_rollout_policy_value, n_jobs=10\n",
    "#                                                             , heuristic_kwargs=heuristic_kwargs)\n",
    "# save_test_results(results_dfs_dict, 'heuristics_evaluation/rollout_policy_value_100_rollouts')\n",
    "\n",
    "# heuristic_kwargs = {'n_rollouts': 1000}\n",
    "# results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_rollout_policy_value, n_jobs=10\n",
    "#                                                             , heuristic_kwargs=heuristic_kwargs)\n",
    "# save_test_results(results_dfs_dict, 'heuristics_evaluation/rollout_policy_value_1000_rollouts')\n",
    "\n",
    "heuristic_kwargs = {'n_rollouts': 10000}\n",
    "results_dfs_dict = test_heuristic_on_all_instances_parallel(voa_df, h_rollout_policy_value, n_jobs=10\n",
    "                                                            , heuristic_kwargs=heuristic_kwargs)\n",
    "save_test_results(results_dfs_dict, 'heuristics_evaluation/rollout_policy_value_10000_rollouts')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_rollout_policy_value_10_rollouts = load_test_results('heuristics_evaluation/rollout_policy_value_10_rollouts')\n",
    "print(calculate_mean_metrics(results_rollout_policy_value_10_rollouts))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_rollout_policy_value_100_rollouts = load_test_results('heuristics_evaluation/rollout_policy_value_100_rollouts')\n",
    "print(calculate_mean_metrics(results_rollout_policy_value_100_rollouts))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_rollout_policy_value_1000_rollouts = load_test_results('heuristics_evaluation/rollout_policy_value_1000_rollouts')\n",
    "print(calculate_mean_metrics(results_rollout_policy_value_1000_rollouts))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_rollout_policy_value_10000_rollouts = load_test_results('heuristics_evaluation/rollout_policy_value_10000_rollouts')\n",
    "print(calculate_mean_metrics(results_rollout_policy_value_10000_rollouts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
